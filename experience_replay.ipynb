{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "\n",
    "import numpy as np\n",
    "import numpy.matlib\n",
    "import matplotlib.pyplot as plt\n",
    "from degree_freedom_queen import *\n",
    "from degree_freedom_king1 import *\n",
    "from degree_freedom_king2 import *\n",
    "from generate_game import *\n",
    "from Chess_env import *\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "size_board = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network class to simplify the use of two networks\n",
    "class Model():\n",
    "    def __init__(self, model=None):\n",
    "        if model:\n",
    "            self.W1 = model.W1\n",
    "            self.W2 = model.W2\n",
    "            self.bias_W1 = model.bias_W1\n",
    "            self.bias_W2 = model.bias_W2\n",
    "        else:\n",
    "            self.W1 = np.random.uniform(0,1,(N_h, N_in))\n",
    "            self.W2 = np.random.uniform(0,1,(N_a, N_h))\n",
    "            self.W1 = np.divide(self.W1,np.matlib.repmat(np.sum(self.W1,1)[:,None],1,N_in))\n",
    "            self.W2 = np.divide(self.W2,np.matlib.repmat(np.sum(self.W2,1)[:,None],1,N_h))\n",
    "            self.bias_W1 = np.zeros((N_h,))\n",
    "            self.bias_W2 = np.zeros((N_a,))\n",
    "    \n",
    "    def set_weights(self, m):\n",
    "        self.W1 = m[0]\n",
    "        self.W2 = m[1]\n",
    "        self.bias_W1 = m[2]\n",
    "        self.bias_W2 = m[3]\n",
    "\n",
    "    def get_weights(self):\n",
    "        return self.W1, self.W2, self.bias_W1, self.bias_W2\n",
    "\n",
    "# Forward pass through neural network to compute Q-values\n",
    "def predict(x0, model):\n",
    "    h1 = np.dot(model.W1, x0) + model.bias_W1\n",
    "    x1 = 1 / (1 + np.exp(-h1))\n",
    "    h2 = np.dot(model.W2, x1) + model.bias_W2\n",
    "    x2 = 1 / (1 + np.exp(-h2))\n",
    "    return x1, x2\n",
    "\n",
    "# Backpropagate error through neural network in batches\n",
    "def backprop_batch(batch_size, x0s, x1s, x2s, x2_targets, model):\n",
    "    dW1 = np.zeros(model.W1.shape)\n",
    "    dW2 = np.zeros(model.W2.shape)\n",
    "    dbias_W1 = np.zeros(model.bias_W1.shape)\n",
    "    dbias_W2 = np.zeros(model.bias_W2.shape)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        delta2 = x2s[i]*(1-x2s[i]) * (x2_targets[i] - x2s[i])\n",
    "        dW2 += np.outer(delta2, x1s[i])\n",
    "        dbias_W2 += delta2\n",
    "\n",
    "        delta1 = x1s[i]*(1-x1s[i]) * np.dot(model.W2.T, delta2)\n",
    "        dW1 += np.outer(delta1, x0s[i])\n",
    "        dbias_W1 += delta1\n",
    "    \n",
    "    model.W1 += eta * dW1 / batch_size\n",
    "    model.W2 += eta * dW2 / batch_size\n",
    "    model.bias_W1 += eta * dbias_W1 / batch_size\n",
    "    model.bias_W2 += eta * dbias_W2 / batch_size\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Epsilon greedy policy\n",
    "def policy(allowed_a, q, epsilon_f):\n",
    "    possible_a = np.where(allowed_a==1)[0]\n",
    "    q_a = q[possible_a]\n",
    "    \n",
    "    if np.random.random() < epsilon_f:\n",
    "        return possible_a[np.random.randint(possible_a.size)]\n",
    "    else:\n",
    "        return possible_a[np.argmax(q_a)]\n",
    "\n",
    "# Sample a batch from experience replay and train the network on it\n",
    "def train(batch_size, replay_memory, Done, main_model, target_model):\n",
    "    if len(replay_memory) < batch_size:\n",
    "        return main_model\n",
    "    \n",
    "    mini_batch = random.sample(replay_memory, batch_size)\n",
    "    current_states, intermediate_states = [], []\n",
    "\n",
    "    new_qs, old_qs = [], []\n",
    "    for X, a, R, Xn, Done in (mini_batch):\n",
    "        x1, q = predict(X, main_model)\n",
    "        current_states.append(X)\n",
    "        intermediate_states.append(x1)\n",
    "        if Done:\n",
    "            target = R\n",
    "        else:\n",
    "            _, qn = predict(Xn, target_model)\n",
    "            target = R + gamma * np.max(qn)\n",
    "        \n",
    "        old_qs.append(q.copy())\n",
    "        q[a] = target\n",
    "        new_qs.append(q)\n",
    "    \n",
    "    return backprop_batch(batch_size, current_states, intermediate_states, old_qs, new_qs, main_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize environment\n",
    "env=Chess_Env(size_board)\n",
    "\n",
    "# Initialize random seed for reproducibility\n",
    "np.random.seed(45) # 41, 42, 43, 44, 45\n",
    "\n",
    "# Initialize neural network paramters\n",
    "S,X,allowed_a=env.Initialise_game()\n",
    "N_a = np.shape(allowed_a)[0] # Output size\n",
    "N_in = np.shape(X)[0] # Input size\n",
    "N_h = 200 # Hidden neurons\n",
    "\n",
    "# Create main network and target network (copy of main)\n",
    "main_model = Model()\n",
    "target_model = Model(main_model)\n",
    "\n",
    "# Hyperparamters\n",
    "epsilon_0 = 0.2\n",
    "beta = 0.00005 # epsilon decay\n",
    "gamma = 0.85 # discount factor\n",
    "eta = 0.0035 # learning rate\n",
    "\n",
    "batch_size = 24\n",
    "replay_memory = deque(maxlen=10000) # experience replay buffer\n",
    "train_rate = 24 # After how many actions a training step is done\n",
    "target_model_update_rate = 100 # After how many action the target network is updated\n",
    "\n",
    "N_episodes = 10000 # number of episodes\n",
    "\n",
    "R_save = np.zeros(N_episodes)\n",
    "N_moves_save = np.zeros(N_episodes)\n",
    "\n",
    "steps_to_update = 0\n",
    "for n in tqdm(range(N_episodes)):\n",
    "\n",
    "    epsilon_f = epsilon_0 / (1 + beta * n) # decay epsilon\n",
    "    Done = 0\n",
    "    i = 1 # number of moves\n",
    "    \n",
    "    _, X, allowed_a = env.Initialise_game()\n",
    "    \n",
    "    while not Done:\n",
    "        steps_to_update += 1\n",
    "        \n",
    "        # Make a move according to policy\n",
    "        x1, q = predict(X, main_model)\n",
    "        a = policy(allowed_a, q, epsilon_f)\n",
    "        _, Xn, allowed_an, R, Done = env.OneStep(a)\n",
    "        # And append the transition to the memory\n",
    "        replay_memory.append([X, a, R, Xn, Done])\n",
    "\n",
    "        # Periodically train the network on a batch from memory\n",
    "        if steps_to_update % train_rate == 0 or Done:\n",
    "            main_model = train(batch_size, replay_memory, Done, main_model, target_model)\n",
    "        \n",
    "        # Update the current state, allowed action and move counter\n",
    "        X = Xn\n",
    "        allowed_a = allowed_an\n",
    "        i += 1\n",
    "    \n",
    "    # When done, save statistics\n",
    "    R_save[n] = R\n",
    "    N_moves_save[n] = i\n",
    "\n",
    "    # Update target model with weights from main model\n",
    "    if Done and steps_to_update >= target_model_update_rate:\n",
    "        target_model.set_weights(main_model.get_weights())\n",
    "        steps_to_update = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example code to store Reward and moves in a file\n",
    "with open(\"R.npy\", \"wb\") as f:\n",
    "    np.save(f, np.array(R_save))\n",
    "\n",
    "with open(\"moves.npy\", \"wb\") as f:\n",
    "    np.save(f, np.array(N_moves_save))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def exp_moving_average(data,alpha):\n",
    "\n",
    "    alpha_rev = 1-alpha\n",
    "\n",
    "    scale = 1/alpha_rev\n",
    "    n = data.shape[0]\n",
    "\n",
    "    r = np.arange(n)\n",
    "    scale_arr = scale**r\n",
    "    offset = data[0]*alpha_rev**(r+1)\n",
    "    pw0 = alpha*alpha_rev**(n-1)\n",
    "\n",
    "    mult = data*pw0*scale_arr\n",
    "    cumsums = mult.cumsum()\n",
    "    out = offset + cumsums*scale_arr[::-1]\n",
    "    return out\n",
    "\n",
    "# Plot exp. moving average of reward and moves\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax1.set_xlabel('Game')\n",
    "ax1.set_ylabel('Reward', color=\"red\")\n",
    "ax1.plot(exp_moving_average(R_save, 1/1000),color=\"red\",label='Reward')\n",
    "ax1.tick_params(axis='y', labelcolor=\"red\")\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "ax2.set_ylabel('Moves',color=\"blue\")\n",
    "ax2.plot(exp_moving_average(N_moves_save, 1/1000),color=\"blue\",label='Moves')\n",
    "ax2.tick_params(axis='y', labelcolor=\"blue\")\n",
    "\n",
    "lines, labels = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax2.legend(lines + lines2, labels + labels2, loc=0)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
