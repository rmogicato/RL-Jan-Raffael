{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import \n",
    "\n",
    "import numpy as np\n",
    "import numpy.matlib\n",
    "import matplotlib.pyplot as plt\n",
    "from degree_freedom_queen import *\n",
    "from degree_freedom_king1 import *\n",
    "from degree_freedom_king2 import *\n",
    "from generate_game import *\n",
    "from Chess_env import *\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "size_board = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the game environment\n",
    "env=Chess_Env(size_board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize random seed for reproducibility\n",
    "np.random.seed(41) # 41, 42, 43, 44, 45\n",
    "\n",
    "# Initialize neural network\n",
    "S,X,allowed_a=env.Initialise_game()\n",
    "\n",
    "N_a = np.shape(allowed_a)[0] # Output size\n",
    "N_in = np.shape(X)[0] # Input size\n",
    "N_h = 200 # Hidden neurons\n",
    "\n",
    "W1 = np.random.uniform(0,1,(N_h, N_in))\n",
    "W2 = np.random.uniform(0,1,(N_a, N_h))\n",
    "W1 = np.divide(W1,np.matlib.repmat(np.sum(W1,1)[:,None],1,N_in))\n",
    "W2 = np.divide(W2,np.matlib.repmat(np.sum(W2,1)[:,None],1,N_h))\n",
    "\n",
    "bias_W1 = np.zeros((N_h,))\n",
    "bias_W2 = np.zeros((N_a,))\n",
    "\n",
    "# Activate RMSprop\n",
    "is_RMSprop = False\n",
    "if is_RMSprop:\n",
    "    squared_gradients = np.zeros(4)\n",
    "\n",
    "# Hyperparamters\n",
    "epsilon_0 = 0.2\n",
    "beta = 0.00005 # epsilon decay\n",
    "gamma = 0.85 # discount factor\n",
    "eta = 0.0035 # learning rate\n",
    "\n",
    "N_episodes = 10000 # number of episodes\n",
    "\n",
    "R_save = np.zeros(N_episodes)\n",
    "N_moves_save = np.zeros(N_episodes)\n",
    "\n",
    "# Forward pass through neural network to compute Q-values\n",
    "def predict(x0, W1, W2, bias_W1, bias_W2):\n",
    "    h1 = np.dot(W1, x0) + bias_W1\n",
    "    x1 = 1 / (1 + np.exp(-h1))\n",
    "    h2 = np.dot(W2, x1) + bias_W2\n",
    "    x2 = 1 / (1 + np.exp(-h2))\n",
    "    return x1, x2\n",
    "\n",
    "# Backpropagate error through neural network\n",
    "def backprop(q, q_err, x0, x1, W1, W2, bias_W1, bias_W2):\n",
    "    delta2 = q*(1-q) * q_err\n",
    "    dW2 = np.outer(delta2, x1)\n",
    "    delta1 = x1*(1-x1) * np.dot(W2.T, delta2)\n",
    "    dW1 = np.outer(delta1, x0)\n",
    "    \n",
    "    W1 += eta * dW1\n",
    "    W2 += eta * dW2\n",
    "    bias_W1 += eta * delta1\n",
    "    bias_W2 += eta * delta2\n",
    "    \n",
    "    return W1, W2, bias_W1, bias_W2\n",
    "\n",
    "# Backpropagate error through neural network using RMSprop\n",
    "def RMSprop(q, q_err, x0, x1, W1, W2, bias_W1, bias_W2, squared_gradients):\n",
    "    delta2 = q*(1-q) * q_err\n",
    "    dW2 = np.outer(delta2, x1)\n",
    "    delta1 = x1*(1-x1) * np.dot(W2.T, delta2)\n",
    "    dW1 = np.outer(delta1, x0)\n",
    "    \n",
    "    rms_beta = 0.9\n",
    "    dW1_squared = rms_beta * squared_gradients[0] + (1-rms_beta) * np.square(dW1) # weighted average of previous squared gradient\n",
    "    dW2_squared = rms_beta * squared_gradients[1] + (1-rms_beta) * np.square(dW2) # and current squared gradient\n",
    "    alpha1 = eta / (1e-6 + np.sqrt(dW1_squared))\n",
    "    alpha2 = eta / (1e-6 + np.sqrt(dW2_squared))\n",
    "    \n",
    "    db1_squared = rms_beta * squared_gradients[2] + (1-rms_beta) * np.square(delta1)\n",
    "    db2_squared = rms_beta * squared_gradients[3] + (1-rms_beta) * np.square(delta2)\n",
    "    alpha1b = eta / (1e-6 + np.sqrt(db1_squared))\n",
    "    alpha2b = eta / (1e-6 + np.sqrt(db2_squared))\n",
    "    \n",
    "    W1 += alpha1 * dW1\n",
    "    W2 += alpha2 * dW2\n",
    "    bias_W1 += alpha1b * delta1\n",
    "    bias_W2 += alpha2b * delta2\n",
    "    \n",
    "    new_squared_gradients = np.array([dW1_squared, dW2_squared, db1_squared, db2_squared], dtype=object)\n",
    "    return W1, W2, bias_W1, bias_W2, new_squared_gradients\n",
    "\n",
    "# Epsilon greedy policy\n",
    "def policy(allowed_a, q, epsilon_f):\n",
    "    possible_a = np.where(allowed_a==1)[0]\n",
    "    q_a = q[possible_a]\n",
    "    \n",
    "    if np.random.random() < epsilon_f:\n",
    "        return possible_a[np.random.randint(possible_a.size)]\n",
    "    else:\n",
    "        return possible_a[np.argmax(q_a)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in tqdm(range(N_episodes)):\n",
    "\n",
    "    epsilon_f = epsilon_0 / (1 + beta * n) # decay epsilon\n",
    "    Done = 0\n",
    "    i = 1 # number of moves\n",
    "    R = 0\n",
    "    \n",
    "    _, X, allowed_a = env.Initialise_game()\n",
    "    \n",
    "    while not Done:\n",
    "        \n",
    "        # Make a move according to policy\n",
    "        x1, q = predict(X, W1, W2, bias_W1, bias_W2)\n",
    "        a = policy(allowed_a, q, epsilon_f)\n",
    "        _, Xn, allowed_an, R, Done = env.OneStep(a)\n",
    "\n",
    "        q_err = np.zeros(N_a)\n",
    "        \n",
    "        if Done:\n",
    "            # Store statistics\n",
    "            R_save[n] = R\n",
    "            N_moves_save[n] = i\n",
    "            \n",
    "            q_err[a] = R - q[a]\n",
    "            if is_RMSprop:\n",
    "                W1, W2, bias_W1, bias_W2, squared_gradients = RMSprop(q, q_err, X, x1, W1, W2, bias_W1, bias_W2, squared_gradients)\n",
    "            else:\n",
    "                W1, W2, bias_W1, bias_W2 = backprop(q, q_err, X, x1, W1, W2, bias_W1, bias_W2)\n",
    "            break\n",
    "        \n",
    "        # Predict next Q-values\n",
    "        _, qn = predict(Xn, W1, W2, bias_W1, bias_W2)\n",
    "        \n",
    "        q_err[a] = (R + gamma * np.max(qn) - q[a])\n",
    "        if is_RMSprop:\n",
    "            W1, W2, bias_W1, bias_W2, squared_gradients  = RMSprop(q, q_err, X, x1, W1, W2, bias_W1, bias_W2, squared_gradients)\n",
    "        else:\n",
    "            W1, W2, bias_W1, bias_W2 = backprop(q, q_err, X, x1, W1, W2, bias_W1, bias_W2)\n",
    "        \n",
    "        # Update the current state, allowed action and move counter\n",
    "        X = Xn\n",
    "        allowed_a = allowed_an\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in tqdm(range(N_episodes)):\n",
    "\n",
    "    epsilon_f = epsilon_0 / (1 + beta * n) # decay epsilon\n",
    "    Done = 0\n",
    "    i = 1 # number of moves\n",
    "    R = 0\n",
    "    \n",
    "    _, X, allowed_a = env.Initialise_game() \n",
    "    x1, q = predict(X, W1, W2, bias_W1, bias_W2) \n",
    "    a = policy(allowed_a, q, epsilon_f) # choose a using policy\n",
    "    \n",
    "    while not Done:\n",
    "        \n",
    "        _, Xn, allowed_an, R, Done = env.OneStep(a) # take action a\n",
    "\n",
    "        q_err = np.zeros(N_a)\n",
    "        \n",
    "        if Done:\n",
    "            R_save[n] = R\n",
    "            N_moves_save[n] = i\n",
    "            \n",
    "            q_err[a] = R - q[a]\n",
    "            if is_RMSprop:\n",
    "                W1, W2, bias_W1, bias_W2, squared_gradients = RMSprop(q, q_err, X, x1, W1, W2, bias_W1, bias_W2, squared_gradients)\n",
    "            else:\n",
    "                W1, W2, bias_W1, bias_W2 = backprop(q, q_err, X, x1, W1, W2, bias_W1, bias_W2)\n",
    "            break\n",
    "        \n",
    "        _, qn = predict(Xn, W1, W2, bias_W1, bias_W2) # get next Q\n",
    "        an = policy(allowed_an, qn, epsilon_f) # choose a' from S' using Q\n",
    "\n",
    "        q_err[a] = R + gamma * qn[an] - q[a]\n",
    "        if is_RMSprop:\n",
    "            W1, W2, bias_W1, bias_W2, squared_gradients  = RMSprop(q, q_err, X, x1, W1, W2, bias_W1, bias_W2, squared_gradients)\n",
    "        else:\n",
    "            W1, W2, bias_W1, bias_W2 = backprop(q, q_err, X, x1, W1, W2, bias_W1, bias_W2)\n",
    "        \n",
    "        # Update the current state, actions and move counter\n",
    "        X = Xn\n",
    "        allowed_a = allowed_an\n",
    "        a = an\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example code to store Reward and moves in a file\n",
    "with open(\"R.npy\", \"wb\") as f:\n",
    "    np.save(f, np.array(R_save))\n",
    "\n",
    "with open(\"moves.npy\", \"wb\") as f:\n",
    "    np.save(f, np.array(N_moves_save))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def exp_moving_average(data,alpha):\n",
    "    alpha_rev = 1-alpha\n",
    "\n",
    "    scale = 1/alpha_rev\n",
    "    n = data.shape[0]\n",
    "\n",
    "    r = np.arange(n)\n",
    "    scale_arr = scale**r\n",
    "    offset = data[0]*alpha_rev**(r+1)\n",
    "    pw0 = alpha*alpha_rev**(n-1)\n",
    "\n",
    "    mult = data*pw0*scale_arr\n",
    "    cumsums = mult.cumsum()\n",
    "    out = offset + cumsums*scale_arr[::-1]\n",
    "    return out\n",
    "\n",
    "# Plot exp. moving average of reward and moves\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax1.set_xlabel('Game')\n",
    "ax1.set_ylabel('Reward', color=\"red\")\n",
    "ax1.plot(exp_moving_average(R_save, 1/1000),color=\"red\",label='Reward')\n",
    "ax1.tick_params(axis='y', labelcolor=\"red\")\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "ax2.set_ylabel('Moves',color=\"blue\")\n",
    "ax2.plot(exp_moving_average(N_moves_save, 1/1000),color=\"blue\",label='Moves')\n",
    "ax2.tick_params(axis='y', labelcolor=\"blue\")\n",
    "\n",
    "lines, labels = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax2.legend(lines + lines2, labels + labels2, loc=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
